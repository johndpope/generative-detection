{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from typing import List, Tuple\n",
    "from pytorch3d.renderer.cameras import get_screen_to_ndc_transform, get_ndc_to_screen_transform\n",
    "from src.util.cameras import PatchPerspectiveCameras, get_patch_ndc_to_ndc_transform, get_ndc_to_patch_ndc_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat sample image batch_size times with different patches of different sizes\n",
    "image_size = [(256, 256), (128, 64), (64, 128), (32, 32)]\n",
    "patch_size = [(64, 64), (32, 32), (16, 16), (8, 8)]\n",
    "\n",
    "# wrt the top left of the full image in pixels\n",
    "patch_center = [(0, 0), (0, 0), (0, 0), (0, 0)]\n",
    "\n",
    "cam_kwargs = {\n",
    "    \"znear\": 0.0,\n",
    "    \"zfar\": 80.0,\n",
    "    \"focal_length\": 10.0,\n",
    "    \"principal_point\": ((0.0, 0.0),),\n",
    "    \"R\": torch.eye(3).unsqueeze(0),\n",
    "    \"T\": torch.zeros(1, 3),\n",
    "    \"device\": \"cpu\",\n",
    "    \"in_ndc\": False,\n",
    "    \"image_size\": image_size\n",
    "    }\n",
    "\n",
    "cam = PatchPerspectiveCameras(**cam_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cam.get_ndc_camera_transform(**cam_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_point_screen tensor([[[0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/n/fs/pci-sharedt/tb21/generative-detection/test_ndc_to_cam.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcycles/n/fs/pci-sharedt/tb21/generative-detection/test_ndc_to_cam.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m world_to_patch_ndc_transform \u001b[39m=\u001b[39m cam\u001b[39m.\u001b[39;49mget_patch_projection_transform(patch_size, patch_center, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcam_kwargs)\n",
      "File \u001b[0;32m/n/fs/pci-sharedt/tb21/generative-detection/src/util/cameras.py:73\u001b[0m, in \u001b[0;36mPatchPerspectiveCameras.get_patch_projection_transform\u001b[0;34m(self, patch_size, patch_center, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_patch_projection_transform\u001b[39m(\u001b[39mself\u001b[39m, patch_size, patch_center, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     72\u001b[0m     world_to_proj_transform \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_projection_transform(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m# camera --> screen\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     screen_to_patch_ndc_transform \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_patch_ndc_camera_transform(patch_size, patch_center, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39m# screen --> patch ndc\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     world_to_patch_ndc_transform \u001b[39m=\u001b[39m world_to_proj_transform\u001b[39m.\u001b[39mcompose(screen_to_patch_ndc_transform) \u001b[39m# camera --> patch ndc\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m world_to_patch_ndc_transform\n",
      "File \u001b[0;32m/n/fs/pci-sharedt/tb21/generative-detection/src/util/cameras.py:161\u001b[0m, in \u001b[0;36mPatchPerspectiveCameras.get_patch_ndc_camera_transform\u001b[0;34m(self, patch_size, patch_center, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39mReturns the transform from camera projection space (screen or NDC) to Patch NDC space.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m    patch_ndc_transform (Transform): The camera transform from screen coordinates to patch NDC coordinates.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m image_size \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mimage_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_image_size())\n\u001b[0;32m--> 161\u001b[0m ndc_to_patch_ndc_transform \u001b[39m=\u001b[39m get_ndc_to_patch_ndc_transform( \u001b[39m# ndc --> patch ndc\u001b[39;49;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39mself\u001b[39;49m, with_xyflip\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, image_size\u001b[39m=\u001b[39;49mimage_size, patch_size\u001b[39m=\u001b[39;49mpatch_size, patch_center\u001b[39m=\u001b[39;49mpatch_center\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[39mreturn\u001b[39;00m ndc_to_patch_ndc_transform\n",
      "File \u001b[0;32m/n/fs/pci-sharedt/tb21/generative-detection/src/util/cameras.py:344\u001b[0m, in \u001b[0;36mget_ndc_to_patch_ndc_transform\u001b[0;34m(cameras, with_xyflip, image_size, patch_size, patch_center)\u001b[0m\n\u001b[1;32m    342\u001b[0m c_point_screen \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([cx_patch, cy_patch, cz_patch], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mc_point_screen\u001b[39m\u001b[39m\"\u001b[39m, c_point_screen)\n\u001b[0;32m--> 344\u001b[0m c_point_ndc \u001b[39m=\u001b[39m screen_to_ndc_transform\u001b[39m.\u001b[39;49mtransform_points(c_point_screen)\n\u001b[1;32m    345\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mc_point_ndc\u001b[39m\u001b[39m\"\u001b[39m, c_point_ndc)\n\u001b[1;32m    346\u001b[0m \u001b[39m# For non square images, we scale the points such that the aspect ratio is preserved.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m# We assume that the image is centered at the origin\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m# patch ndc --> ndc\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m# send to available device\u001b[39;00m\n",
      "File \u001b[0;32m/n/fs/pci-sharedt/tb21/miniconda3/envs/inrdetect4/lib/python3.10/site-packages/pytorch3d/transforms/transform3d.py:395\u001b[0m, in \u001b[0;36mTransform3d.transform_points\u001b[0;34m(self, points, eps)\u001b[0m\n\u001b[1;32m    392\u001b[0m points_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([points_batch, ones], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    394\u001b[0m composed_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_matrix()\n\u001b[0;32m--> 395\u001b[0m points_out \u001b[39m=\u001b[39m _broadcast_bmm(points_batch, composed_matrix)\n\u001b[1;32m    396\u001b[0m denom \u001b[39m=\u001b[39m points_out[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m:]  \u001b[39m# denominator\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mif\u001b[39;00m eps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/n/fs/pci-sharedt/tb21/miniconda3/envs/inrdetect4/lib/python3.10/site-packages/pytorch3d/transforms/transform3d.py:827\u001b[0m, in \u001b[0;36m_broadcast_bmm\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(b) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    826\u001b[0m         b \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mexpand(\u001b[39mlen\u001b[39m(a), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 827\u001b[0m \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39;49mbmm(b)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "world_to_patch_ndc_transform = cam.get_patch_projection_transform(patch_size, patch_center, **cam_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_ndc_to_world_transform = world_to_patch_ndc_transform.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_patch_ndc = torch.tensor([[0.5, 0.5, 1.0], [0.25, 0.25, 1.0], [0.75, 0.75, 1.0], [1., 1., 1.0]])\n",
    "X_world = patch_ndc_to_world_transform.transform_points(X_patch_ndc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_to_patch_ndc_transform = cam.get_patch_projection_transform(patch_size, patch_center, **cam_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_patch_ndc = world_to_patch_ndc_transform.transform_points(X_world)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat sample image batch_size times with different patches of different sizes\n",
    "image_size = [(256, 256)]\n",
    "patch_size = [(128, 128)]\n",
    "\n",
    "# wrt corner of the full image in pixels\n",
    "patch_center = [(128, 128)]\n",
    "\n",
    "cam_kwargs = {\n",
    "    \"znear\": 0.0,\n",
    "    \"zfar\": 80.0,\n",
    "    \"focal_length\": 1.0,\n",
    "    \"principal_point\": ((0.0, 0.0),),\n",
    "    \"R\": torch.eye(3).unsqueeze(0),\n",
    "    \"T\": torch.zeros(1, 3),\n",
    "    \"device\": \"cpu\",\n",
    "    \"in_ndc\": False,\n",
    "    \"image_size\": image_size\n",
    "    }\n",
    "\n",
    "cam = PatchPerspectiveCameras(**cam_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topleft, topcenter, topright, midleft, midcenter, midright, botleft, botcenter, botright (-1,-1, 1) to (1, 1, 1)\n",
    "patch_ndc_points = [(-1.0, -1.0, 1.0), (0.0, -1.0, 1.0), (1.0, -1.0, 1.0),\n",
    "                    (-1.0, 0.0, 1.0), (0.0, 0.0, 1.0), (1.0, 0.0, 1.0),\n",
    "                    (-1.0, 1.0, 1.0), (0.0, 1.0, 1.0), (1.0, 1.0, 1.0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5000, -0.5000,  1.0000],\n",
       "        [ 0.0000, -0.5000,  1.0000],\n",
       "        [ 0.5000, -0.5000,  1.0000],\n",
       "        [-0.5000,  0.0000,  1.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000],\n",
       "        [ 0.5000,  0.0000,  1.0000],\n",
       "        [-0.5000,  0.5000,  1.0000],\n",
       "        [ 0.0000,  0.5000,  1.0000],\n",
       "        [ 0.5000,  0.5000,  1.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_ndc_to_ndc_transform = get_patch_ndc_to_ndc_transform(cameras=cam, \n",
    "                                                            image_size=image_size, \n",
    "                                                            patch_size=patch_size, \n",
    "                                                            patch_center=patch_center)\n",
    "ndc_points = patch_ndc_to_ndc_transform.transform_points(torch.tensor(patch_ndc_points))\n",
    "ndc_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1.,  1.],\n",
       "        [ 0., -1.,  1.],\n",
       "        [ 1., -1.,  1.],\n",
       "        [-1.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [-1.,  1.,  1.],\n",
       "        [ 0.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_ndc_revert_transform = get_ndc_to_patch_ndc_transform(cameras=cam,\n",
    "                                                            image_size=image_size, \n",
    "                                                            patch_size=patch_size, \n",
    "                                                            patch_center=patch_center)\n",
    "patch_ndc_points_revert = patch_ndc_revert_transform.transform_points(ndc_points)\n",
    "patch_ndc_points_revert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-192., -192.,    1.],\n",
       "        [-128., -192.,    1.],\n",
       "        [ -64., -192.,    1.],\n",
       "        [-192., -128.,    1.],\n",
       "        [-128., -128.,    1.],\n",
       "        [ -64., -128.,    1.],\n",
       "        [-192.,  -64.,    1.],\n",
       "        [-128.,  -64.,    1.],\n",
       "        [ -64.,  -64.,    1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndc_to_screen_transform = get_ndc_to_screen_transform(cameras=cam, image_size=image_size)\n",
    "screen_points = ndc_to_screen_transform.transform_points(ndc_points)      \n",
    "screen_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_points = cam.unproject_points(screen_points) # screen --> world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-192., -192.,    1.],\n",
       "        [-128., -192.,    1.],\n",
       "        [ -64., -192.,    1.],\n",
       "        [-192., -128.,    1.],\n",
       "        [-128., -128.,    1.],\n",
       "        [ -64., -128.,    1.],\n",
       "        [-192.,  -64.,    1.],\n",
       "        [-128.,  -64.,    1.],\n",
       "        [ -64.,  -64.,    1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[192., 192.,   1.],\n",
       "        [128., 192.,   1.],\n",
       "        [ 64., 192.,   1.],\n",
       "        [192., 128.,   1.],\n",
       "        [128., 128.,   1.],\n",
       "        [ 64., 128.,   1.],\n",
       "        [192.,  64.,   1.],\n",
       "        [128.,  64.,   1.],\n",
       "        [ 64.,  64.,   1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen_points_revert = cam.transform_points_screen(world_points)\n",
    "screen_points_revert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5000, -0.5000,  1.0000],\n",
       "        [ 0.0000, -0.5000,  1.0000],\n",
       "        [ 0.5000, -0.5000,  1.0000],\n",
       "        [-0.5000,  0.0000,  1.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000],\n",
       "        [ 0.5000,  0.0000,  1.0000],\n",
       "        [-0.5000,  0.5000,  1.0000],\n",
       "        [ 0.0000,  0.5000,  1.0000],\n",
       "        [ 0.5000,  0.5000,  1.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndc_points_revert = cam.transform_points_ndc(world_points)\n",
    "ndc_points_revert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
